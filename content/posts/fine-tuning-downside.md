+++
title = "The Downside of Streamlined Fine-Tuning for Frontier Labs"
date = 2025-10-22
draft = false
+++

## Origins of the Thought
With the rise of fine-tuning as a service, I have been reflecting on fine-tuning in general. Recently, I read a paper about a group at Morgan Stanley that fine-tuned Qwen models to perform well at writing Q code. Q is a programming language used in high-performance finance, but it sees limited adoption elsewhere, resulting in limited data.

Initial benchmarking revealed that the models underperformed, likely due to the scarcity of training data and their inability to meet specific needs.

## The Allure of Simplified Services
This experience led me to consider services like [OpenAI's Reinforcement Fine-Tuning (RFT)](https://platform.openai.com/docs/guides/reinforcement-fine-tuning), where users create a "grader file," leverage a reasoning model, and iteratively improve performance based on a deterministic grader. Their website features case studies from companies like [Harvey](https://www.harvey.ai/).

However, after reviewing the [nearly 40-page technical report from the Morgan Stanley group](https://arxiv.org/abs/2508.06813), I concluded that the easier it becomes to fine-tune a proprietary model, the less likely sophisticated users will adopt it for specialized cases.

This may seem counterproductive, but fine-tuning is far from straightforward. I initially assumed it involved collecting reference code, performing basic pre-training, and calling it complete. The reality proves otherwise.

The paper highlights numerous pitfalls, from dataset construction and evaluation design to issues like reward hacking in Q problems and solutions.

## The Rigors of the Process
Once the dataset is prepared, supervised fine-tuning requires training the generator model on an expanding corpus. Midway, pre-training intervenes to counteract generation stagnation. Finally, pass metrics are implemented, often incorporating reinforcement learning.

The team iterated extensively on each step to refine the process, as failure points abound.

Even with RFT, users must still develop the grader file and evaluation harness in advance; the service does not eliminate these prerequisites.

Organizations pursue fine-tuning for exceptional performance on a single task. Research consistently shows that smaller models rival larger ones for specialized domains like coding. In a large language model, most parameters remain inactive for narrow applications. LLMs excel through versatility across tasks. Yet, when fine-tuning targets one domain, the scale of a massive model becomes unnecessary.

## Economic and Practical Incentives

If a fine tuning harness is already in place, why subsidize cloud inference costs from providers like Anthropic or OpenAI? Inference APIs represent one of the few profitable segments for frontier labs, with strong margins.

For entities with their own GPUs, a smaller open-source model offers superior marginal benefits, especially since the foundational setup for fine-tuning already exists.

I question the strategy of simplifying fine-tuning for proprietary GPTs. For sophisticated users, the opposite effect may occur, favoring independent, tailored solutions.

*Transcribed from a voice memo using Gemini 2.5 Pro.*
